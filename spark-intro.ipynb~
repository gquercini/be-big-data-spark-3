{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='./figs/cs-logo.png' width=200></center>\n",
    "\n",
    "\n",
    "\n",
    "<h6><center>Big Data: Informatique pour les donn√©es et calculs massifs</center></h6>\n",
    "\n",
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Lab Assignment 3: Introduction to Spark</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction (PLEASE READ ME)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In this lab assignment you'll learn basic Spark programming skills that are necessary to develop simple, yet powerful, applications to be executed in a distributed environment.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The assignment is presented in this __Jupyter Notebook__, an interface that offers support for text, code, images and other media. Essentially, a Jupyter Notebook consists of multiple _cells_, either containing some text, like the one that you are reading, or code that you can execute. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The cells that contain code are marked with the label \"In [  ]\" that is found on the left of the cell. In order to execute the code in the cell, you need to select the cell and press Shift+Enter (hold Shift while pressing Enter). During the execution of the code, the label on the left of the cell will change to \"In [ * ]\"; when the execution is over, the asterisk is replaced by a number. IMPORTANT: wait for the execution of the cell to be over before proceeding in the Notebook. \n",
    "Whenever you define a variable or a function in a cell, that variable and function will be visible in the cells below. \n",
    "This means that one can split the code of an application across different cells to interleave it with textual explanations.\n",
    "    </font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Spark supports four programming languages: Scala (the one used to implement Spark itself), Java, R and Python. In this assignment we use Python.\n",
    "The assignment will guide you through the Spark programming notions by using simple examples. \n",
    "After the examples, the exercises will give you the chance to practice those notions.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "**Apache Spark** is a cluster computing framework for parallel data processing that was conceived to address the inefficiencies of Hadoop with respect to iterative computations. \n",
    "Spark is used by both data scientists, who analyze large datasets, and engineers, who develop data processing applications. Spark allows both to concentrate on their application by hiding all the complexity of running applications in a distributed environment: distributed systems programming, network communication and fault tolerance.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In order to run a distributed computation on Spark, one has to develop a **Spark application**.\n",
    "A Spark application runs as a set of independent processes (called the _Executors_) across the machines (a.k.a., _Worker_ nodes) of a cluster, coordinated by the _Driver_, the process that runs the $main()$ function of the application.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The _Driver_ creates an object called _SparkContext_ that communicates with the underlying cluster manager and coordinates the distribution of the computation across the _Executors_.\n",
    "For example, if we were running an application to count the number of lines in a file, \n",
    "different machines might count lines in different ranges of the file. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<img src=\"./figs/spark-execution.png\" width=400>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font  size=\"3\" color='#91053d'>**Execute the following cell in order to initialize the _SparkContext_.**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization successful\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"td3\")\n",
    "print(\"Initialization successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Resilient Distributed Dataset (RDD)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Spark distributes the data and the computations across the machines of a cluster by using the notion of **Resilient Distributed Dataset (RDD)**. \n",
    "An RDD is an immutable distributed collection of data. \n",
    "Each element of an RDD can be an instance of any Python type, including a user-defined class.\n",
    "The _SparkContext_  splits an RDD into multiple _partitions_ and scatters them across different machines of the cluster. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The distribution of the partitions of an RDD is completely transparent to the application.\n",
    "The only thing a Spark application has to do is to create some RDDs and \n",
    "specify the computations on these RDDs, by using special functions that Spark provides to this purpose. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "## 2.1 Creating RDDs\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Spark provides two ways to create an RDD:\n",
    "\n",
    "<ul>\n",
    "<li> By distributing a collection of objects.\n",
    "<li> By loading an external dataset (either in a file or a database).\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to create an RDD called $words$, where each element is a string taken from the list $wordList$.**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "wordList = [\"Al\", \"Ani\", \"Jackie\", \"Lalitha\", \"Mark\", \"Neil\", \"Nick\", \"Shirin\"]\n",
    "words = sc.parallelize(wordList)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Once created, Spark provides two types of  operations on a RDD:\n",
    "<ol>\n",
    "<li> **Transformations**. A transformation takes in one or more RDDs and returns a new RDD.\n",
    "<li> **Actions**. An action takes in an RDD and outputs a value.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "One common transformation is filtering data that matches a predicate by using the function $filter$.\n",
    "The function $filter$ is applied on an RDD and takes in a predicate.\n",
    "It loops through each element of the RDD and verifies whether that element satisfies the predicate. The function $filter$ outputs a new RDD whose elements are those that satisfy the predicate.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to create an RDD called $nNames$ by retaining only the names whose first letter is 'N' from the RDD $words$ .**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As mentioned before, the function filter takes in a predicate that is itself a function (returning a boolean value).\n",
    "An elegant way to pass a function to a function in Python is the lambda notation.\n",
    "\n",
    "In the code below, the argument of the function filter is a function that takes in a variable called \n",
    "\"name\"; the type of this variable must match the type of the elements of the RDD words, in this case \n",
    "a string.\n",
    "Then the function returns whether the first character of the string \"name\" is N. \n",
    "\n",
    "Another way to express the same thing without the lambda notation would be to explicitly define the predicate, \n",
    "as follows:\n",
    "\n",
    "def predicate(name):\n",
    "    return name[:1]=='N'\n",
    "\n",
    "nNames = words.filter(predicate)\n",
    "\n",
    "'''\n",
    "nNames = words.filter(lambda name: name[:1]=='N') \n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The transformation $filter$ above is not executed by Spark until an action is called on the RDD $nNames$.\n",
    "In general, Spark postpones the execution of a transformation on an RDD to when an action is invoked on the RDD itself. This is called _lazy evaluation_. The reason for this approach will be clearer in the next example.\n",
    "    </font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "One example of action is the function $first()$ that returns the first element in the RDD.\n",
    "    </font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to print the first element of the RDD $nNames$.**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neil\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "REMEMBER: the variable nNames has been defined in the cell above, so \n",
    "it is VISIBLE in this cell as well as in the cells below the current one\n",
    "'''\n",
    "print(nNames.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The previous example clearly shows that a Spark application is just a sequence of operations that create, transform and perform some actions on RDDs.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The following code shows an example of creation of RDD from an external dataset, more precisely a text file that contains log information of a Neo4j database. \n",
    "The function $textFile()$ takes in the path to the input text file and returns an RDD, where each element is a line of the file.\n",
    "The code goes through the following steps:\n",
    "\n",
    "<ol>\n",
    "<li> **RDD creation**. Creates an RDD called $lines$, where each element is a line from the input text file.\n",
    "<li> **RDD filter**. Creates an RDD called $exceptions$ from the RDD $lines$ by only retaining the lines containing the string \"exception\".\n",
    "<li> **Action count()**. Counts the number of elements of the RDD $exceptions$. \n",
    "<li> **Print first line**. Prints the first line of the RDD $exceptions$. \n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Here we can see the advantage of the _lazy evaluation_ of transformations. \n",
    "Spark does not execute the function $textFile()$ as soon as it is invoked, which would result\n",
    "in loading into main memory the whole content of the log file (it can be very large). \n",
    "Instead, it waits until the first action $count()$ is invoked.\n",
    "Since this action is called on a filtered version of the RDD $lines$, \n",
    "Spark will load into memory only the lines that contain the word \"exception\".\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code.**</font>\n",
    " <hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exception lines  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2017-10-06 13:12:26.097+0000 ERROR Failed to start Neo4j: Starting Neo4j failed: Component \\'org.neo4j.server.database.LifecycleManagingDatabase@2814f71a\\' was successfully initialized, but failed to start. Please see the attached cause exception \"Format version is not supported (resource BufferedChecksumIndexInput(MMapIndexInput(path=\"/Users/quercini_gia/Documents/software/neo4j-community-3.2.5/data/databases/social-network/upgrade/index/lucene/relationship/crosslinks/segments_1\"))): -11 (needs to be between 1071082519 and 1071082519). This version of Lucene only supports indexes created with release 4.0 and later.\". Starting Neo4j failed: Component \\'org.neo4j.server.database.LifecycleManagingDatabase@2814f71a\\' was successfully initialized, but failed to start. Please see the attached cause exception \"Format version is not supported (resource BufferedChecksumIndexInput(MMapIndexInput(path=\"/Users/quercini_gia/Documents/software/neo4j-community-3.2.5/data/databases/social-network/upgrade/index/lucene/relationship/crosslinks/segments_1\"))): -11 (needs to be between 1071082519 and 1071082519). This version of Lucene only supports indexes created with release 4.0 and later.\".']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"./data/neo4j.log\"\n",
    "\n",
    "# 1. RDD creation\n",
    "lines = sc.textFile(data_file)\n",
    "\n",
    "#2. RDD filter\n",
    "exceptions = lines.filter(lambda line : \"exception\" in line)\n",
    "\n",
    "#3. Action count()\n",
    "nbLines = exceptions.count()\n",
    "print(\"Number of exception lines \", nbLines)\n",
    "\n",
    "#4. Print first line.\n",
    "exceptions.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 2.2 Transformations\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Here are some common transformations in Spark. \n",
    "In the following list, $r$ denotes the RDD on which the transformation is invoked.\n",
    "<ol>\n",
    "<li> $r.filter()$. Returns an RDD consisting of only the elements of the input RDD $r$ that satisfy a predicate.\n",
    "<li> $r.map(func)$. Applies a function $func$ to each element of the input RDD $r$ and returns an RDD of the result.\n",
    "<li> $r.flatMap(func)$. Same as $map()$, but used when $map()$ would return an RDD where each element is a list.\n",
    "<li> $r.union(other)$. Takes in two RDDs ($r$ and $other$) and returns an RDD that contains the elements from both. Unlike the mathematical operation, $union$ in Spark does not remove the duplicates.\n",
    "<li> $r.intersection(other)$. Takes in two RDDs ($r$ and $other$) and returns an RDD that contains the elements found in both.\n",
    "<li> $r.subtract(other)$. Takes in two RDDs ($r$ and $other$) and returns an RDD that contains the elements from the RDD $r$, except those that are found in $other$.\n",
    "<li> $r.cartesian(other)$. Takes in two RDDs ($r$ and $other$) and returns an RDD that contains the Cartesian product of both.\n",
    "<li> $r.distinct()$. Returns an RDD that contains the same elements as the input RDD $r$ without duplicates.\n",
    "</ol>\n",
    "    \n",
    "We are now going to look at an example of use of these transformations.\n",
    "</font>    \n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to create the two RDDs $r1$ and $r2$**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "r1 = sc.parallelize([1, 2, 3, 4])\n",
    "r2 = sc.parallelize([3, 4, 5, 6, 7])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of $map()$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Here we see an example of use of the transformation $map()$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to create:\n",
    " <ol>\n",
    " <li> an RDD $square$, where each element is the square of the corresponding element in $r1$; \n",
    " <li> an RDD $half$, where each element is the half of the corresponding element in $r2$.**\n",
    " </ol>\n",
    " </font>\n",
    " <hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of RDD square  [1, 4, 9, 16]\n",
      "Elements of the RDD half  [1.5, 2.0, 2.5, 3.0, 3.5]\n"
     ]
    }
   ],
   "source": [
    "square = r1.map(lambda x : x*x)\n",
    "half = r2.map(lambda x: x/2)\n",
    "\n",
    "# The function collect() is an action that transforms the RDD into a Python list that can be printed.\n",
    "print(\"Elements of RDD square \", square.collect())\n",
    "print(\"Elements of the RDD half \", half.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of $flatMap()$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The transformation $flatMap()$ works by applying the transformation $map()$ on the input RDD; \n",
    "if each element of the resulting RDD is a list, $flatMap()$ returns an RDD where all lists are merged.\n",
    "In other words, when $map()$ returns an RDD where the elements are lists, $flatMap()$ returns an RDD where the elements are the values of the list.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Let's see an example. Suppose that we want to return an RDD from $r1$ where each element is associated to its square. \n",
    "More precisely, the output RDD will be as follows:\n",
    "<center>\n",
    "[ [1, 1], [2, 4], [3, 9], [4, 16] ]\n",
    "</center>\n",
    "</font>    \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to create an RDD $squares$ where each element of $r1$ is associated to its square.**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of RDD squares  [[1, 1], [2, 4], [3, 9], [4, 16]]\n"
     ]
    }
   ],
   "source": [
    "squares = r1.map(lambda x : [x, x*x])\n",
    "print(\"Elements of RDD squares \", squares.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "\n",
    "As you can see, each element of the RDD $squares$ is a list of two elements; indeed, \n",
    "after calling the action $collect()$, we obtain a list of lists in Python. \n",
    "In order to obtain a simple list of elements, we flatten the lists with $flatMap()$.\n",
    "More precisely, with $flatMap()$ we obtain the following RDD:\n",
    "<center>\n",
    "[1, 1, 2, 4, 3, 9, 4, 16]\n",
    "</center>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to create an RDD $squares$ where each element of $r1$ is associated to its square (but each element is just a value instead of a list of values)**</font>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of RDD squares  [1, 1, 2, 4, 3, 9, 4, 16]\n"
     ]
    }
   ],
   "source": [
    "squares = r1.flatMap(lambda x : [x, x*x])\n",
    "print(\"Elements of RDD squares \", squares.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of set transformations\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The set transformations are $union$, $intersection$, $subtract$ and $cartesian$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to see an example of these transformations**</font>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of RDD r1  [1, 2, 3, 4]\n",
      "Elements of RDD r2  [3, 4, 5, 6, 7]\n",
      "Elements of RDD union  [1, 2, 3, 4, 3, 4, 5, 6, 7]\n",
      "Elements of RDD intesection  [3, 4]\n",
      "Elements of RDD subtract  [1, 2]\n",
      "Elements of RDD cartesian  [(1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7)]\n"
     ]
    }
   ],
   "source": [
    "union = r1.union(r2)\n",
    "intersection = r1.intersection(r2)\n",
    "subtract = r1.subtract(r2)\n",
    "cartesian = r1.cartesian(r2)\n",
    "\n",
    "print(\"Elements of RDD r1 \", r1.collect())\n",
    "print(\"Elements of RDD r2 \", r2.collect())\n",
    "print(\"Elements of RDD union \", union.collect())\n",
    "print(\"Elements of RDD intesection \", intersection.collect())\n",
    "print(\"Elements of RDD subtract \", subtract.collect())\n",
    "print(\"Elements of RDD cartesian \", cartesian.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of $distinct()$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The transformation $distinct()$ returns an RDD that contains the same elements as the input RDD without the duplicates.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    " <font size=\"3\" color='#91053d'>**Execute the following code to see an example of use of $distinct()$**</font>\n",
    " <hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of the RDD union:  [1, 2, 3, 4, 3, 4, 5, 6, 7]\n",
      "Elements of the RDD union (with no duplicates):  [1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "nodup = union.distinct()\n",
    "print(\"Elements of the RDD union: \", union.collect())\n",
    "print(\"Elements of the RDD union (with no duplicates): \", nodup.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 2.3. Actions\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Here are some common actions in Spark. \n",
    "As with transformations, $r$ denotes the RDD on which the action is invoked.\n",
    "\n",
    "<ol>\n",
    "<li> $r.reduce(func)$. Performs a pair-wise application of the given function $func$ to the elements of the input RDD $r$.\n",
    "<li> $r.collect()$. Returns a list with all the elements of the input RDD $r$.\n",
    "<li> $r.count()$. Returns the number of elements in the input RDD $r$.\n",
    "<li> $r.countByValue()$. Returns the number of times each element occurs in the input RDD $r$.\n",
    "<li> $r.take(num)$. Prints the first $num$ elements of the input RDD $r$.\n",
    "<li> $r.top(num)$. Prints the top $num$ elements of the input RDD $r$ (sorted in decreasing order).\n",
    "</ol>\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of $reduce(func)$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The action $reduce(func)$ performs a pair-wise application of the given function $func$ on the elements of the input RDD. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to sum all values of the RDD $r1$**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of the RDD r1  [1, 2, 3, 4]\n",
      "Sum of the elements of the RDD r1:  10\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The function passed to the reduce MUST take in two arguments \n",
    "that have the same type as the elements of the input RDD\n",
    "'''\n",
    "sum = r1.reduce(lambda x, y : x + y)\n",
    "print(\"Elements of the RDD r1 \", r1.collect())\n",
    "print(\"Sum of the elements of the RDD r1: \", sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of $countByValue()$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The action $countByValue()$ counts the number of the occurrences of each element of the input RDD.\n",
    "The result is a Python dictionary, where a key is an element of the input RDD and the corresponding value the number of its occurrences.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to get the number of occurrences of each element in the RDD $union$**</font>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of the RDD union  [1, 2, 3, 4, 3, 4, 5, 6, 7]\n",
      "Occurrences of each element in the RDD union:\n",
      "1  -->  1  occurrences\n",
      "2  -->  1  occurrences\n",
      "3  -->  2  occurrences\n",
      "4  -->  2  occurrences\n",
      "5  -->  1  occurrences\n",
      "6  -->  1  occurrences\n",
      "7  -->  1  occurrences\n"
     ]
    }
   ],
   "source": [
    "occurrences = union.countByValue()\n",
    "print(\"Elements of the RDD union \", union.collect())\n",
    "print(\"Occurrences of each element in the RDD union:\")\n",
    "for k, v in occurrences.items():\n",
    "    print(k, \" --> \",  v, \" occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercises \n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Now it's your turn! \n",
    "You're going to apply yourself the transformations and the actions that we've seen so far. \n",
    "To this extent, we will use the file **./data/moby-dick.txt** that contains the text of the famous novel by Herman Melville. \n",
    "The goal of this exercise is to count the number of words in the file and the number of occurrences of each word.\n",
    "</font>\n",
    "</p>\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "First, we are going to preprocess the input text, by removing punctuation and special characters and to lowercase all words.\n",
    "</font>\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "###  Exercise 1\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The function $preprocess()$ defined below takes in an RDD that contains the lines of the input text file and returns an RDD where each element is a word. In the code below, the first step of the function $preprocess()$, which removes the non-letter characters from the input RDD by using a regular expression, is already implemented.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>\n",
    "**Complete the function $proprocess()$ with the following steps:**\n",
    "<ol start=\"2\">\n",
    "<li>    Apply a transformation on the RDD $text$ to obtain an RDD $words$, where each element is a word from the file.\n",
    "<li>    Filter out from the RDD $words$ the words with length 0.\n",
    "<li>    Lowercase all words of the RDD $words$.\n",
    "<li>    Return the RDD $words$.\n",
    "</ol>\n",
    "Execute the code, which will print the number of words in the input text file and the top-100 most frequent words.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  208240\n",
      "Top-100 most frequent words\n",
      "the ---> 14065\n",
      "of ---> 6440\n",
      "and ---> 6257\n",
      "a ---> 4535\n",
      "to ---> 4491\n",
      "in ---> 4050\n",
      "that ---> 2900\n",
      "his ---> 2485\n",
      "it ---> 2343\n",
      "i ---> 1930\n",
      "but ---> 1763\n",
      "he ---> 1729\n",
      "as ---> 1698\n",
      "with ---> 1690\n",
      "is ---> 1672\n",
      "was ---> 1618\n",
      "for ---> 1574\n",
      "all ---> 1464\n",
      "this ---> 1351\n",
      "at ---> 1295\n",
      "by ---> 1169\n",
      "not ---> 1121\n",
      "from ---> 1071\n",
      "him ---> 1035\n",
      "so ---> 1030\n",
      "on ---> 1028\n",
      "be ---> 1024\n",
      "whale ---> 891\n",
      "one ---> 875\n",
      "you ---> 851\n",
      "had ---> 765\n",
      "now ---> 759\n",
      "there ---> 755\n",
      "have ---> 751\n",
      "or ---> 686\n",
      "were ---> 674\n",
      "they ---> 639\n",
      "which ---> 617\n",
      "then ---> 614\n",
      "me ---> 606\n",
      "some ---> 604\n",
      "their ---> 603\n",
      "when ---> 591\n",
      "my ---> 584\n",
      "are ---> 584\n",
      "an ---> 583\n",
      "like ---> 567\n",
      "no ---> 566\n",
      "upon ---> 560\n",
      "what ---> 537\n",
      "into ---> 517\n",
      "out ---> 509\n",
      "more ---> 499\n",
      "up ---> 496\n",
      "if ---> 469\n",
      "its ---> 457\n",
      "them ---> 450\n",
      "old ---> 436\n",
      "man ---> 433\n",
      "we ---> 426\n",
      "would ---> 421\n",
      "ahab ---> 417\n",
      "ye ---> 416\n",
      "been ---> 408\n",
      "over ---> 397\n",
      "other ---> 393\n",
      "these ---> 387\n",
      "ship ---> 378\n",
      "will ---> 377\n",
      "only ---> 369\n",
      "such ---> 364\n",
      "sea ---> 363\n",
      "whales ---> 363\n",
      "though ---> 362\n",
      "down ---> 358\n",
      "yet ---> 339\n",
      "who ---> 330\n",
      "time ---> 321\n",
      "her ---> 321\n",
      "any ---> 320\n",
      "very ---> 317\n",
      "long ---> 313\n",
      "still ---> 307\n",
      "about ---> 305\n",
      "those ---> 302\n",
      "than ---> 300\n",
      "do ---> 297\n",
      "captain ---> 292\n",
      "before ---> 291\n",
      "great ---> 289\n",
      "said ---> 288\n",
      "has ---> 287\n",
      "here ---> 284\n",
      "seemed ---> 280\n",
      "must ---> 279\n",
      "two ---> 278\n",
      "last ---> 273\n",
      "most ---> 273\n",
      "head ---> 263\n",
      "thou ---> 262\n"
     ]
    }
   ],
   "source": [
    "import operator;\n",
    "import re;\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    Regular expression for removing all non-letter characters in the file.\n",
    "    '''\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    '''\n",
    "    Step 1. \n",
    "    Remove the non-letter characters.\n",
    "    After this transformation, mobydick is an RDD where each element is a line of the file\n",
    "    '''\n",
    "    text = text.map(lambda line: regex.sub('', line))\n",
    "    \n",
    "    '''\n",
    "    Step 2.\n",
    "    Obtain the RDD containing the words in the file.\n",
    "    Note that the function line.split() returns the list of words in a line. \n",
    "    Therefore, each element of the RDD words would be a list, shouldn't we use flatMap.\n",
    "    '''\n",
    "    words = text.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "    '''\n",
    "    Step 3.\n",
    "    Filter out the words with length 0.\n",
    "    '''\n",
    "    words = words.filter(lambda word: len(word) > 0)\n",
    "    \n",
    "    '''\n",
    "    Step 4.\n",
    "    Lowercase all words of the RDD words\n",
    "    '''\n",
    "    words = words.map(lambda word: word.lower())\n",
    "    \n",
    "    '''\n",
    "    Step 5.\n",
    "    Returns the RDD words.\n",
    "    '''\n",
    "    return words\n",
    "\n",
    "\n",
    "# Reads the novel into a RDD\n",
    "mobydick = sc.textFile('./data/moby-dick.txt')\n",
    "\n",
    "words = preprocess(mobydick)\n",
    "\n",
    "print(\"Number of words \", words.count())\n",
    "\n",
    "'''\n",
    "Top-100 most frequent words\n",
    "'''\n",
    "# countByValue() counts the number of occurrences of each word.\n",
    "# It returns a dictionary. \n",
    "# The function sorted sorts by value and returns a list of tuples (w, f), where w is a word \n",
    "# and f its number of occurrences.\n",
    "# The list will be sorted in ascending order of number of occurrences\n",
    "occurrences = sorted(words.countByValue().items(), key=operator.itemgetter(1))\n",
    "\n",
    "# Reverse the order\n",
    "occurrences.reverse()\n",
    "print(\"Top-100 most frequent words\")\n",
    "i = 0\n",
    "for (w, f) in occurrences:\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break\n",
    "    print(w,\"--->\",f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In the previous example, you've probably noticed that the top-100 most frequent words are _functions_ words, such as articles, pronouns, conjunctions. These words have little lexical meaning and express a grammatical relationship with the other words. \n",
    "In many applications, these functions words are not useful and they can be ignored.\n",
    "For instance, when you submit a query against a search engine (e.g., \"restaurants in Paris\"), function words (e.g., \"in\") are not useful to retrieve the Web pages relevant to the query.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In text processing applications, function words are part of a list of _stop words_ that include all words that are frequently used and give little semantic contribution to the content of a text.\n",
    "Since there isn't any agreement as to the definition of _stop word_, it's possible to find many such lists on the Web.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a function $preprocess()$ that applies the same operations as the function that you defined above and, in addition, removes the stop words. This function $preprocess()$ takes in: (i) an RDD that contains the lines of the input text file (as above) and (ii) an RDD containing the stop words. The function returns an RDD with the words of the input text file. **</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "HINT: The file with the list of stop words is **./data/stopwords.txt**. All stop words are already lowercased.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after stopword removal  102358\n",
      "Top-100 most frequent words\n",
      "whale ,  891\n",
      "one ,  875\n",
      "old ,  436\n",
      "man ,  433\n",
      "ahab ,  417\n",
      "ye ,  416\n",
      "ship ,  378\n",
      "sea ,  363\n",
      "whales ,  363\n",
      "though ,  362\n",
      "time ,  321\n",
      "long ,  313\n",
      "captain ,  292\n",
      "great ,  289\n",
      "said ,  288\n",
      "seemed ,  280\n",
      "must ,  279\n",
      "two ,  278\n",
      "last ,  273\n",
      "head ,  263\n",
      "see ,  257\n",
      "way ,  253\n",
      "little ,  247\n",
      "white ,  247\n",
      "boat ,  241\n",
      "round ,  239\n",
      "three ,  235\n",
      "sperm ,  232\n",
      "first ,  229\n",
      "stubb ,  227\n",
      "men ,  224\n",
      "every ,  223\n",
      "say ,  223\n",
      "us ,  221\n",
      "well ,  221\n",
      "much ,  218\n",
      "queequeg ,  211\n",
      "good ,  195\n",
      "hand ,  194\n",
      "go ,  183\n",
      "side ,  183\n",
      "thing ,  181\n",
      "look ,  178\n",
      "boats ,  175\n",
      "made ,  174\n",
      "away ,  173\n",
      "chapter ,  172\n",
      "come ,  170\n",
      "starbuck ,  169\n",
      "many ,  161\n",
      "water ,  158\n",
      "deck ,  158\n",
      "far ,  157\n",
      "seen ,  156\n",
      "day ,  156\n",
      "eyes ,  153\n",
      "ships ,  152\n",
      "sir ,  151\n",
      "sort ,  151\n",
      "cried ,  149\n",
      "back ,  148\n",
      "thought ,  147\n",
      "world ,  147\n",
      "part ,  147\n",
      "know ,  145\n",
      "whole ,  137\n",
      "oh ,  137\n",
      "right ,  136\n",
      "aye ,  135\n",
      "life ,  134\n",
      "crew ,  133\n",
      "air ,  133\n",
      "thus ,  132\n",
      "night ,  128\n",
      "tell ,  128\n",
      "soon ,  128\n",
      "thee ,  128\n",
      "came ,  126\n",
      "take ,  126\n",
      "hands ,  125\n",
      "things ,  125\n",
      "feet ,  123\n",
      "small ,  122\n",
      "pequod ,  121\n",
      "till ,  119\n",
      "something ,  118\n",
      "think ,  116\n",
      "line ,  116\n",
      "god ,  115\n",
      "thy ,  113\n",
      "found ,  113\n",
      "towards ,  113\n",
      "full ,  111\n",
      "along ,  110\n",
      "times ,  110\n",
      "dont ,  110\n",
      "another ,  110\n",
      "nothing ,  109\n",
      "make ,  109\n",
      "whaling ,  108\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text, stopwords):\n",
    "    '''\n",
    "    Regular expression for removing all non-letter characters in the file.\n",
    "    '''\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    '''\n",
    "    Step 1. \n",
    "    Remove the non-letter characters.\n",
    "    After this transformation, mobydick is an RDD where each element is a line of the file\n",
    "    '''\n",
    "    text = text.map(lambda line: regex.sub('', line))\n",
    "    \n",
    "    '''\n",
    "    Step 2.\n",
    "    Obtain the RDD containing the words in the file.\n",
    "    Note that the function line.split() returns the list of words in a line. \n",
    "    Therefore, each element of the RDD words would be a list, shouldn't we use flatMap.\n",
    "    '''\n",
    "    words = text.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "    '''\n",
    "    Step 3.\n",
    "    Filter out the words with length 0.\n",
    "    '''\n",
    "    words = words.filter(lambda word: len(word) > 0)\n",
    "    \n",
    "    '''\n",
    "    Step 4.\n",
    "    Lowercase all words of the RDD $words$\n",
    "    '''\n",
    "    words = words.map(lambda word: word.lower())\n",
    "    \n",
    "    '''\n",
    "    Step 5 Remove stop words.\n",
    "    '''\n",
    "    words = words.subtract(stopwords)\n",
    "    \n",
    "    # Returns the words\n",
    "    return words\n",
    "\n",
    "\n",
    "# Load the stopwords to an RDD\n",
    "stopwords = sc.textFile(\"./data/stopwords.txt\")\n",
    "words = preprocess(mobydick, stopwords)\n",
    "print(\"Number of words after stopword removal \", words.count())\n",
    "\n",
    "# Count the number of occurrences as before.\n",
    "occurrences = sorted(words.countByValue().items(), key=operator.itemgetter(1))\n",
    "# Reverse the order\n",
    "occurrences.reverse()\n",
    "print(\"Top-100 most frequent words\")\n",
    "i = 0\n",
    "for (w, f) in occurrences:\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break\n",
    "    print(w, \", \", f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pair RDDs \n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Pair RDDs are simply RDDs where each element is a key-value pair. They are useful for expressing _MapReduce_ computations. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We are now going to look at examples of  transformations and actions on pair RDDs.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Creation of Pair RDDs\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "One common way to create a pair RDD is to transform an existing RDD with a $map()$. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to create a Pair RDD $kvwords$ from the RDD $words$. Each element of $kvwords$ is a pair where the key is a word and the value is 1**</font>\n",
    "</p>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('timberheads', 1), ('timberheads', 1), ('parchmentlike', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('hailed', 1), ('unfractioned', 1), ('adoration', 1), ('adoration', 1), ('acted', 1), ('acted', 1), ('acted', 1), ('hebrew', 1), ('hebrew', 1), ('hebrew', 1), ('hillock', 1), ('otherunless', 1), ('holes', 1), ('holes', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('grown', 1), ('skilful', 1), ('skilful', 1), ('waycut', 1), ('bigotry', 1), ('girth', 1), ('girth', 1), ('girth', 1), ('girth', 1), ('girth', 1), ('girth', 1), ('guineas', 1), ('basement', 1), ('basement', 1), ('hadst', 1), ('solitaries', 1), ('solitaries', 1), ('anacharsis', 1), ('talks', 1), ('wailing', 1), ('nowthe', 1), ('nowthe', 1), ('feminine', 1), ('feminine', 1), ('proving', 1), ('upliftings', 1), ('preparations', 1), ('preparations', 1), ('shadeowing', 1), ('remunerative', 1), ('snowy', 1), ('snowy', 1), ('snowy', 1), ('snowy', 1), ('snowy', 1), ('snowy', 1), ('snowy', 1), ('snowy', 1), ('turnpike', 1), ('yarman', 1), ('yarman', 1), ('yarman', 1), ('yarman', 1), ('yarman', 1), ('disappearing', 1), ('disappearing', 1), ('disappearing', 1), ('downtown', 1), ('affect', 1), ('affect', 1)]\n"
     ]
    }
   ],
   "source": [
    "kvwords = words.map(lambda word : (word, 1))\n",
    "print(kvwords.take(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Transformations on Pair RDDs\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "All transformations applied to standard RDDs can be applied to Pair RDDs as well. The only difference is that any function that is passed to a transformation must take in tuples instead of single values.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In addition, the following transformations can **only** be applied to Pair RDDs:\n",
    "<ol>\n",
    "<li> $r.reduceByKey(func)$. It applies the given function $func$ pairwise to all elements of the input RDD $r$ that are associated to the same key. \n",
    "<li> $r.sortBy(func, asc)$. Returns an RDD where the elements of the input RDD $r$ are sorted according to the given criteria.\n",
    "<li> $r.groupByKey()$. Groups values with the same key from the input RDD $r$.\n",
    "<li> $r.keys()$. Returns an RDD where the elements are the keys from the input RDD $r$.\n",
    "<li> $r.values()$. Returns an RDD where the elements are the values from the input RDD $r$.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to count the number of occurrences of each word in the RDD $kvwords$ and print the top-100 most frequent words**</font>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whale', 891),\n",
       " ('one', 875),\n",
       " ('old', 436),\n",
       " ('man', 433),\n",
       " ('ahab', 417),\n",
       " ('ye', 416),\n",
       " ('ship', 378),\n",
       " ('whales', 363),\n",
       " ('sea', 363),\n",
       " ('though', 362),\n",
       " ('time', 321),\n",
       " ('long', 313),\n",
       " ('captain', 292),\n",
       " ('great', 289),\n",
       " ('said', 288),\n",
       " ('seemed', 280),\n",
       " ('must', 279),\n",
       " ('two', 278),\n",
       " ('last', 273),\n",
       " ('head', 263),\n",
       " ('see', 257),\n",
       " ('way', 253),\n",
       " ('little', 247),\n",
       " ('white', 247),\n",
       " ('boat', 241),\n",
       " ('round', 239),\n",
       " ('three', 235),\n",
       " ('sperm', 232),\n",
       " ('first', 229),\n",
       " ('stubb', 227),\n",
       " ('men', 224),\n",
       " ('say', 223),\n",
       " ('every', 223),\n",
       " ('us', 221),\n",
       " ('well', 221),\n",
       " ('much', 218),\n",
       " ('queequeg', 211),\n",
       " ('good', 195),\n",
       " ('hand', 194),\n",
       " ('go', 183),\n",
       " ('side', 183),\n",
       " ('thing', 181),\n",
       " ('look', 178),\n",
       " ('boats', 175),\n",
       " ('made', 174),\n",
       " ('away', 173),\n",
       " ('chapter', 172),\n",
       " ('come', 170),\n",
       " ('starbuck', 169),\n",
       " ('many', 161),\n",
       " ('deck', 158),\n",
       " ('water', 158),\n",
       " ('far', 157),\n",
       " ('seen', 156),\n",
       " ('day', 156),\n",
       " ('eyes', 153),\n",
       " ('ships', 152),\n",
       " ('sir', 151),\n",
       " ('sort', 151),\n",
       " ('cried', 149),\n",
       " ('back', 148),\n",
       " ('part', 147),\n",
       " ('world', 147),\n",
       " ('thought', 147),\n",
       " ('know', 145),\n",
       " ('oh', 137),\n",
       " ('whole', 137),\n",
       " ('right', 136),\n",
       " ('aye', 135),\n",
       " ('life', 134),\n",
       " ('crew', 133),\n",
       " ('air', 133),\n",
       " ('thus', 132),\n",
       " ('night', 128),\n",
       " ('thee', 128),\n",
       " ('tell', 128),\n",
       " ('soon', 128),\n",
       " ('came', 126),\n",
       " ('take', 126),\n",
       " ('hands', 125),\n",
       " ('things', 125),\n",
       " ('feet', 123),\n",
       " ('small', 122),\n",
       " ('pequod', 121),\n",
       " ('till', 119),\n",
       " ('something', 118),\n",
       " ('think', 116),\n",
       " ('line', 116),\n",
       " ('god', 115),\n",
       " ('thy', 113),\n",
       " ('towards', 113),\n",
       " ('found', 113),\n",
       " ('full', 111),\n",
       " ('times', 110),\n",
       " ('dont', 110),\n",
       " ('another', 110),\n",
       " ('along', 110),\n",
       " ('nothing', 109),\n",
       " ('make', 109),\n",
       " ('whaling', 108)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "kvwords is an RDD where each element is (w, 1), w is a word from the novel Moby Dick.\n",
    "If a word w occurs k times, there will be k pairs (w, 1) in the RDD kvwords.\n",
    "To compute the number of occurrences of each word, \n",
    "we should just sum the 1s associated to all pairs with the same key, for all keys.\n",
    "This is achieved with the reduceByKey() function.\n",
    "'''\n",
    "occurrences = kvwords.reduceByKey(lambda x, y : x + y)\n",
    "\n",
    "'''\n",
    "Each element of the RDD occurrences is a pair (w, f), where w is a word and f is the number of occurrences of w.\n",
    "Finally, we sort the RDD occurrences by value by using sortBy(). \n",
    "The first argument of the function sortBy() is a function that takes in a key-value pair and returns the \n",
    "value (x[1]). \n",
    "The second argument specifies the order (ascending or descending).\n",
    "'''\n",
    "occurrences = occurrences.sortBy(lambda x: x[1], ascending=False)\n",
    "occurrences.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Another way to obtain the same result, would be to:\n",
    "<ol>\n",
    "<li> Apply the transformation $groupByKey()$ to the RDD $kvwords$. The result is a RDD $occurrences$, where each element is a pair $(w, L)$, $w$ is a word and $L$ is a list of 1s (as many as the number of occurrences of $w$).\n",
    "<li> Apply a transformation $map()$ to the RDD $occurrences$ to obtain a new RDD $occurrences$, where each element is a pair $(w, len(L))$, $w$ being a word and $len(L)$ being the number of elements of $L$.\n",
    "<li> Sort the RDD $occurrences$ as before.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code to see the result with this alternative solution**</font>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whale', 891),\n",
       " ('one', 875),\n",
       " ('old', 436),\n",
       " ('man', 433),\n",
       " ('ahab', 417),\n",
       " ('ye', 416),\n",
       " ('ship', 378),\n",
       " ('whales', 363),\n",
       " ('sea', 363),\n",
       " ('though', 362),\n",
       " ('time', 321),\n",
       " ('long', 313),\n",
       " ('captain', 292),\n",
       " ('great', 289),\n",
       " ('said', 288),\n",
       " ('seemed', 280),\n",
       " ('must', 279),\n",
       " ('two', 278),\n",
       " ('last', 273),\n",
       " ('head', 263),\n",
       " ('see', 257),\n",
       " ('way', 253),\n",
       " ('little', 247),\n",
       " ('white', 247),\n",
       " ('boat', 241),\n",
       " ('round', 239),\n",
       " ('three', 235),\n",
       " ('sperm', 232),\n",
       " ('first', 229),\n",
       " ('stubb', 227),\n",
       " ('men', 224),\n",
       " ('say', 223),\n",
       " ('every', 223),\n",
       " ('us', 221),\n",
       " ('well', 221),\n",
       " ('much', 218),\n",
       " ('queequeg', 211),\n",
       " ('good', 195),\n",
       " ('hand', 194),\n",
       " ('go', 183),\n",
       " ('side', 183),\n",
       " ('thing', 181),\n",
       " ('look', 178),\n",
       " ('boats', 175),\n",
       " ('made', 174),\n",
       " ('away', 173),\n",
       " ('chapter', 172),\n",
       " ('come', 170),\n",
       " ('starbuck', 169),\n",
       " ('many', 161),\n",
       " ('deck', 158),\n",
       " ('water', 158),\n",
       " ('far', 157),\n",
       " ('seen', 156),\n",
       " ('day', 156),\n",
       " ('eyes', 153),\n",
       " ('ships', 152),\n",
       " ('sir', 151),\n",
       " ('sort', 151),\n",
       " ('cried', 149),\n",
       " ('back', 148),\n",
       " ('part', 147),\n",
       " ('world', 147),\n",
       " ('thought', 147),\n",
       " ('know', 145),\n",
       " ('oh', 137),\n",
       " ('whole', 137),\n",
       " ('right', 136),\n",
       " ('aye', 135),\n",
       " ('life', 134),\n",
       " ('crew', 133),\n",
       " ('air', 133),\n",
       " ('thus', 132),\n",
       " ('night', 128),\n",
       " ('thee', 128),\n",
       " ('tell', 128),\n",
       " ('soon', 128),\n",
       " ('came', 126),\n",
       " ('take', 126),\n",
       " ('hands', 125),\n",
       " ('things', 125),\n",
       " ('feet', 123),\n",
       " ('small', 122),\n",
       " ('pequod', 121),\n",
       " ('till', 119),\n",
       " ('something', 118),\n",
       " ('think', 116),\n",
       " ('line', 116),\n",
       " ('god', 115),\n",
       " ('thy', 113),\n",
       " ('towards', 113),\n",
       " ('found', 113),\n",
       " ('full', 111),\n",
       " ('times', 110),\n",
       " ('dont', 110),\n",
       " ('another', 110),\n",
       " ('along', 110),\n",
       " ('nothing', 109),\n",
       " ('make', 109),\n",
       " ('whaling', 108)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurrences = kvwords.groupByKey()\n",
    "occurrences = occurrences.map(lambda x : (x[0], len(x[1])))\n",
    "occurrences = occurrences.sortBy(lambda x: x[1], ascending=False)\n",
    "occurrences.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Actions on Pair RDDs\n",
    "\n",
    "<p justify=\"align\">\n",
    "<font size=\"3\">\n",
    "As with the transformations, all actions available for standard RDDs can be used on Pair RDDs as well.\n",
    "In addition, the following actions can be performed on Pair RDDs:\n",
    "<ol>\n",
    "<li> $countByKey()$. Counts the number of values associated with the same key. Returns a dictionary.\n",
    "<li> $collecAsMap()$. Collects the RDD as a dictionary (in the same way as the function $collect()$ returns a list from a standard RDD).\n",
    "<li> $lookup(key)$. Returns a list with all the values associated with the given _key_.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p justify=\"align\">\n",
    "<font size=\"3\">\n",
    "Referring again to the example of counting the occurrences of each word, the following code is yet another way to solve the problem by using the action $countByKey()$ on the RDD $kvwords$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "<font size=\"3\" color='#91053d'>**Execute the following code**</font>\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 most frequent words\n",
      "whale ,  891\n",
      "one ,  875\n",
      "old ,  436\n",
      "man ,  433\n",
      "ahab ,  417\n",
      "ye ,  416\n",
      "ship ,  378\n",
      "sea ,  363\n",
      "whales ,  363\n",
      "though ,  362\n",
      "time ,  321\n",
      "long ,  313\n",
      "captain ,  292\n",
      "great ,  289\n",
      "said ,  288\n",
      "seemed ,  280\n",
      "must ,  279\n",
      "two ,  278\n",
      "last ,  273\n",
      "head ,  263\n",
      "see ,  257\n",
      "way ,  253\n",
      "little ,  247\n",
      "white ,  247\n",
      "boat ,  241\n",
      "round ,  239\n",
      "three ,  235\n",
      "sperm ,  232\n",
      "first ,  229\n",
      "stubb ,  227\n",
      "men ,  224\n",
      "every ,  223\n",
      "say ,  223\n",
      "us ,  221\n",
      "well ,  221\n",
      "much ,  218\n",
      "queequeg ,  211\n",
      "good ,  195\n",
      "hand ,  194\n",
      "go ,  183\n",
      "side ,  183\n",
      "thing ,  181\n",
      "look ,  178\n",
      "boats ,  175\n",
      "made ,  174\n",
      "away ,  173\n",
      "chapter ,  172\n",
      "come ,  170\n",
      "starbuck ,  169\n",
      "many ,  161\n",
      "water ,  158\n",
      "deck ,  158\n",
      "far ,  157\n",
      "seen ,  156\n",
      "day ,  156\n",
      "eyes ,  153\n",
      "ships ,  152\n",
      "sir ,  151\n",
      "sort ,  151\n",
      "cried ,  149\n",
      "back ,  148\n",
      "thought ,  147\n",
      "world ,  147\n",
      "part ,  147\n",
      "know ,  145\n",
      "whole ,  137\n",
      "oh ,  137\n",
      "right ,  136\n",
      "aye ,  135\n",
      "life ,  134\n",
      "crew ,  133\n",
      "air ,  133\n",
      "thus ,  132\n",
      "night ,  128\n",
      "tell ,  128\n",
      "soon ,  128\n",
      "thee ,  128\n",
      "came ,  126\n",
      "take ,  126\n",
      "hands ,  125\n",
      "things ,  125\n",
      "feet ,  123\n",
      "small ,  122\n",
      "pequod ,  121\n",
      "till ,  119\n",
      "something ,  118\n",
      "think ,  116\n",
      "line ,  116\n",
      "god ,  115\n",
      "thy ,  113\n",
      "found ,  113\n",
      "towards ,  113\n",
      "full ,  111\n",
      "along ,  110\n",
      "times ,  110\n",
      "dont ,  110\n",
      "another ,  110\n",
      "nothing ,  109\n",
      "make ,  109\n",
      "whaling ,  108\n"
     ]
    }
   ],
   "source": [
    "occurrences = sorted(kvwords.countByKey().items(), key=operator.itemgetter(1))\n",
    "# Reverse the order\n",
    "occurrences.reverse()\n",
    "print(\"Top-100 most frequent words\")\n",
    "i = 0\n",
    "for (w, f) in occurrences:\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break\n",
    "    print(w, \", \", f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exercises\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Now it's your turn to apply the notions that you've learned so far to create some more complex applications.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "##  Exercise 3\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In the folder _./data/bbc_ you'll find a collection of 50 documents from the BBC news website corresponding to stories in five topics from 2004-2005. The five topics are: _business_, _entertainment_, _politics_, _sport_ and _tech_. \n",
    "In the directory, the stories are text files (named _001.txt_, _002.txt_...) organized into five directories, one for topic.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In this exercise, we want to create an **inverted index**, one that associates each word to the list of files the word occurs in.\n",
    "More precisely, for each word, the inverted index will have a list of the names  of the files (path relative to the folder _./data_) that contain the word. The figure below shows the entry in the index for the word \"family\".\n",
    "\n",
    "<img src=\"./figs/inverted-index.png\" width=400>\n",
    "    An inverted index is an essential component of a search engine. In fact, given any word, the inverted index allows the search engine to quickly retrieve all documents containing that word.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The idea is to use a **MapReduce** schema. \n",
    "The **Map** task will create a key-value pair $(w, d)$ for each word $w$ that appears in the document $d$. $d$ is the path of the document relative to the folder _./data_ (e.g., _./data/bbc/business/001.txt_).\n",
    "The **Reduce** task will group by key all the key-value pairs with the same key to form the inverted index.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a Spark application to create an inverted index for the BBC collection. By using the function $lookup$ (already provided), print the documents that contain the word 'family'.**\n",
    "<br>\n",
    "HINT:\n",
    "<ol>\n",
    "<li> Your program needs to go through all the documents in each of the five folders of the BBC collection. Use the function $getFiles()$ to get the list of all filenames in the collection.\n",
    "<li> To preprocess the text of a document, you can use the function $preprocess()$ defined above. The RDD $words$ returned by this function contains a word as many times as it appears in a document. Since for an inverted index we just need to know that a word is in a specific document, you should remove the duplicates from the RDD $words$.\n",
    "</ol>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following documents contain the word  family\n",
      "./data/bbc/entertainment/005.txt\n",
      "./data/bbc/entertainment/002.txt\n",
      "./data/bbc/entertainment/003.txt\n",
      "./data/bbc/politics/001.txt\n",
      "./data/bbc/sport/004.txt\n",
      "./data/bbc/tech/006.txt\n",
      "./data/bbc/tech/004.txt\n"
     ]
    }
   ],
   "source": [
    "import operator;\n",
    "import re;\n",
    "import os;\n",
    "\n",
    "\n",
    "'''\n",
    "This function is given.\n",
    "It takes in the invertex index and a word and returns \n",
    "the list of documents where the word occurs.\n",
    "'''\n",
    "def lookup(iindex, word):\n",
    "    print(\"The following documents contain the word \", word)\n",
    "    ld = iindex.lookup(word)[0]\n",
    "    for d in ld:\n",
    "        print(d)\n",
    "\n",
    "'''\n",
    "Get all the textual files.\n",
    "'''\n",
    "def getFiles():\n",
    "    files=[]\n",
    "    # The five topics (also, the names of the subdirectories under directory bbc)\n",
    "    topics = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "    for topic in topics:\n",
    "    # Current directory\n",
    "        cd = \"./data/bbc/\"+topic+\"/\"\n",
    "        # For each document under the current topic\n",
    "        for document in os.listdir(cd):\n",
    "            # Get the name of the current document\n",
    "            filename = cd + os.fsdecode(document)\n",
    "            # This is to ensure that  we do not get any hidden file by mistake \n",
    "            if not filename.endswith(\".txt\"):\n",
    "                continue\n",
    "            files.append(filename)\n",
    "    return files\n",
    "\n",
    "'''\n",
    "First step: Map part. \n",
    "Create a Pair RDD, where each element (w, d) is such that w is a word and d is the \n",
    "filename of the document that contains the word.\n",
    "'''\n",
    "\n",
    "stopwords = sc.textFile(\"./data/stopwords.txt\")\n",
    "\n",
    "# Initialize the pair RDD.\n",
    "kv = sc.parallelize([])\n",
    "\n",
    "# Get all files\n",
    "files = getFiles()\n",
    "\n",
    "for file in files:\n",
    "    # Obtain an RDD with the textual content of the document\n",
    "    text = sc.textFile(file);\n",
    "    # Preproces the text. Obtain the words\n",
    "    words = preprocess(text, stopwords);\n",
    "    # Remove the duplicates\n",
    "    words = words.distinct()\n",
    "    # Create a Pair RDD, where each pair maps a word w to the current document\n",
    "    words = words.map(lambda x : (x, file))\n",
    "    # Merge this Pair RDD with kv\n",
    "    kv = kv.union(words)\n",
    "        \n",
    "# Finally, we group by key the pairs in the RDD kv to obtain the inverted index        \n",
    "iindex = kv.groupByKey()\n",
    "\n",
    "# We look up the word \"family\" in the inverted index:\n",
    "lookup(iindex, \"family\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "##  Exercise 4\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Given the BBC collection, we want to calculate the **co-occurrence matrix** $M$, such that $M[w_1][w_2]$ is the number of documents in which two words $w_1$ and $w_2$ appear together.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "A co-occurrence matrix is generally sparse (i.e., lot of entries are zero), because we expect the pairs of words that have no co-occurrence to be far more than the ones that have at least one co-occurrence. It is certainly wise to represent only the non-zero elements of the matrix.\n",
    "To this extent, we use a **MapReduce** schema.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "For any two words $w_i$, $w_j$ that co-occur in the same document, the **Map** task will output a key-value pair $((w_i, w_j), 1)$, where the key is the pair of co-occurring words $(w_i, w_j)$ and the value is 1.\n",
    "Note that the two key-value pairs $((w_i, w_j), 1)$ and $((w_j, w_i), 1)$ ought to be considered as having the same key, because the order of occurrence of the two words does not matter (the only thing that matters is whether the two words co-occur in the same document). \n",
    "For this reason, the key-value pair $((w_i, w_j), 1)$ returned by the Map task is such that $w_i < w_j$, where $<$ is the lexicographic order.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In the output of the Map task, a key-value pair $((w_i, w_j), 1)$ appears as many times as documents where $w_i$ and $w_j$ co-occur. In order to obtain the number of co-occurrences of $w_i$ and $w_j$, the **Reduce** task will just have to sum the values associated to the key $(w_i, w_j)$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a Spark application that creates the co-occurrence matrix of the documents in the BBC collection. Print the top-20 most frequent co-occurrences.**\n",
    "<br>\n",
    "HINT: as before, use the function $preprocess()$ and remove the duplicate words qnd the function $getFiles()$ to get the list of all documents in the collection.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('also', 'said'), 24),\n",
       " (('said', 'world'), 20),\n",
       " (('new', 'said'), 19),\n",
       " (('said', 'year'), 17),\n",
       " (('also', 'world'), 17),\n",
       " (('one', 'said'), 16),\n",
       " (('last', 'said'), 15),\n",
       " (('said', 'set'), 15),\n",
       " (('said', 'time'), 15),\n",
       " (('said', 'years'), 14)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Function that implements the Map task.\n",
    "Takes in: the list of words from a document d.\n",
    "Returns: the list containing the key-value pairs ((w_i, w_j), 1) for all words (w_i, w_j) in the document d.\n",
    "'''\n",
    "def Map(words):\n",
    "    kvpairs = []\n",
    "    for i in range(0, len(words)):\n",
    "        for j in range(i+1, len(words)):  \n",
    "            wi = words[i]\n",
    "            wj = words[j]\n",
    "            # we must have wi < wj\n",
    "            if wi > wj:\n",
    "                temp = wi\n",
    "                wi = wj\n",
    "                wj = temp\n",
    "            kvpairs.append(((wi, wj), 1))\n",
    "    return kvpairs\n",
    "            \n",
    "'''\n",
    "Function that implements the Reduce task.\n",
    "Takes in: the RDD containing all the key-value pairs output by the Map task.\n",
    "Returns: the RDD with the co-occurrence matrix.\n",
    "'''\n",
    "def Reduce(kv):\n",
    "    return kv.reduceByKey(lambda x, y : x + y)\n",
    "\n",
    "# The stop words\n",
    "stopwords = sc.textFile(\"./data/stopwords.txt\")\n",
    "\n",
    "files = getFiles()\n",
    "\n",
    "# Initialize the RDD that will contain the pairs output by the Map task.\n",
    "kv = sc.parallelize([])\n",
    "# Iterate over all topics\n",
    "for file in files:\n",
    "    # Obtain an RDD with the textual content of the document\n",
    "    text = sc.textFile(file)\n",
    "    # Preproces the text. Obtain the words\n",
    "    words = preprocess(text, stopwords)\n",
    "    # Remove the duplicates\n",
    "    words = words.distinct()\n",
    "    # Invoke the map task and appends the output of the map to the RDD kv\n",
    "    kv = kv.union(sc.parallelize(Map(words.collect())))\n",
    "\n",
    "# Reduce task\n",
    "matrix = Reduce(kv)\n",
    "matrix = matrix.sortBy(lambda x: x[1], ascending=False)\n",
    "matrix.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "##  Exercise 5\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "One important task in the analysis of textual data is the computation of the similarity between two textual documents.\n",
    "Intuitively, the more words two textual documents share, the more similar they are.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Let $d_1$ and $d_2$ be two documents, and let $W(d_1)$ and $W(d_2)$ be the set of words in $d_1$ and $d_2$, respectively. The similarity $S(d_1, d_2)$ between $d_1$ and $d_2$ can be computed with the _Jaccard_ score as follows:\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "$$S(d_1, d_2) = \\frac{W(d_1) \\cap W(d_2)}{W(d_1) \\cup W(d_2)}$$\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a function $jaccard$ that takes in two RDDs containing the words of two documents and returns the Jaccard similarity score of the documents. Test the function on the documents of the BBC.**\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity  1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The Jaccard score.\n",
    "Takes in: two RDDs with the words of two documents d1, d2\n",
    "Returns: the Jaccard similarity of the two documents.\n",
    "'''\n",
    "def jaccard(d1w, d2w):\n",
    "    intersect = d1w.intersection(d2w)\n",
    "    union = d1w.union(d2w).distinct()\n",
    "    sim = (intersect.count() / union.count())    \n",
    "    return sim\n",
    "\n",
    "# The stop words\n",
    "stopwords = sc.textFile(\"./data/stopwords.txt\")\n",
    "\n",
    "# The two input documents.\n",
    "d1 = sc.textFile(\"./data/bbc/politics/001.txt\")\n",
    "d2 = sc.textFile(\"./data/bbc/politics/001.txt\")\n",
    "\n",
    "# Preproces\n",
    "d1w = preprocess(d1, stopwords)\n",
    "d1w = d1w.distinct()\n",
    "d2w = preprocess(d2, stopwords)\n",
    "d2w = d2w.distinct()\n",
    "\n",
    "sim = jaccard(d1w, d2w)\n",
    "\n",
    "print(\"similarity \", sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "##  Exercise 6\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We are now going to use the similarity function that we defined in Exercise 4 to create a simple text classifier.\n",
    "A _text classifier_ is an application that takes in a textual document and determines the topic that the document covers.\n",
    "Usually, text classification is based on supervised machine learning algorithms, like Naive Bayes and SVM; \n",
    "since these algorithms are out of the scope of this course, we limit ourselves to a simple classifier that assigns a document to the topic with the highest similarity to the document itself.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "More precisely, let's consider five documents $d_1, \\ldots, d_5$, one for each topic of the BBC dataset; arbitrarily, we say that each of these documents is representative of one topic. \n",
    "Now, consider a document $d$ that is different from the representative documents. \n",
    "The text classifier computes the similarity of $d$ to $d_1, \\ldots, d_5$, selects the document $d_i$ that has the highest similarity to $d$ and assigns $d$ to the topic covered by $d_i$.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a function $classify$ that takes in a document $d$ (already preprocessed) and the list of representative documents (already preprocessed) for each topic and outputs the predicted topic of $d$.**\n",
    "</font>\n",
    "</p>\n",
    "<p>\n",
    "<font size=\"3\" color='#91053d'>\n",
    "HINT: You can use the files named _001.txt_ as the representative documents for each topic.   \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted topic  business\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Classification function.\n",
    "Takes in: RDD d of the preprocessed input document and RDDs of the preprocessed representative documents.\n",
    "Returns: the predicted topic of the input document.\n",
    "'''\n",
    "def classify(d, representative):\n",
    "    max = 0\n",
    "    topic = \"null\"\n",
    "    for r in representative:\n",
    "            sim = jaccard(d, r[0])\n",
    "            if sim > max :\n",
    "                max = sim\n",
    "                topic = r[1]\n",
    "    return topic\n",
    "\n",
    "\n",
    "# The stop words\n",
    "stopwords = sc.textFile(\"./data/stopwords.txt\")\n",
    "\n",
    "'''\n",
    "The representative documents.\n",
    "Each item of this list is a pair (path, topic), associating a path of a document to the name of its topic.\n",
    "'''\n",
    "representative = [(\"./data/bbc/business/001.txt\", \"business\"), \n",
    "                  (\"./data/bbc/entertainment/001.txt\", \"entertainment\"), \n",
    "                  (\"./data/bbc/politics/001.txt\", \"politics\"), \n",
    "                  (\"./data/bbc/sport/001.txt\", \"sport\"),\n",
    "                  (\"./data/bbc/tech/001.txt\", \"tech\")]\n",
    "\n",
    "# The RRDs with the preprocessed representative documents \n",
    "repwords = []\n",
    "\n",
    "# Pre-process the representative documents\n",
    "for rep in representative:\n",
    "    repwords.append((preprocess(sc.textFile(rep[0]), stopwords).distinct(),rep[1]))\n",
    "    \n",
    "\n",
    "\n",
    "# Classify a document.\n",
    "topic = classify(preprocess(sc.textFile(\"./data/bbc/business/001.txt\"), stopwords).distinct(), repwords)\n",
    "print(\"Predicted topic \", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr style=\" border:none; height:2px;\">\n",
    "\n",
    "##  Exercise 7 (Optional part)\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "\n",
    "After implementing a classifier, we need to evaluate it. The evaluation is based on the **confusion matrix** that describes the performance of the classifier on a test dataset for which the true values are known.\n",
    "An example of confusion matrix is shown in the figure below. \n",
    "Each row contains the instances in a predicted class while each column represents the instances in an actual class.\n",
    "From the matrix in the Figure, we learn that there are 10 documents that are in the class \"business\" (see the total of the first column), of which 5 are correctly predicted as being in the class \"business\", 2 are incorrectly classified in the class \"entertainment\" and  3 are incorrectly classified in the class \"sport\".\n",
    "</font>\n",
    "<img src=\"./figs/confusion-matrix.png\" width=800>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "For each class $C$, we can define the notion of true positives (TP), false positives (FP) and false negatives (FN) for that class (the figure shows the TP, FP and FN for the class \"Business\"):\n",
    "<ul>\n",
    "<li> **True positives** (TP). Set of documents whose actual and predicted class is $C$.\n",
    "<li> **False positives** (FP). Set of documents whose predicted class is $C$, while the actual class is not $C$.\n",
    "<li> **False negatives** (FN). Set of documents whose predicted class is not $C$, while the actual class is $C$.\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "For each class $C$, we can compute three evaluation measures, called _precision_ ($P_C$), _recall_ ($R_C$) and \n",
    "_f-measure_ ($F_C$), as follows:\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "$$P_C = \\frac{|TP|}{|TP| + |FP|}\\quad R_C = \\frac{|TP|}{|TP| + |FN|}\\quad F_C = 2\\cdot \\frac{P_C\\cdot R_C}{P_C + R_C}$$\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The _precision_ $P_C$ tells how many of the documents classified in $C$ are correctly classified;\n",
    "the _recall_ $R_C$ tells how many of the documents that are actually in $C$ are correctly classified in $C$;\n",
    "the _f-measure_ is an harmonic mean of _precision_ and _recall_ and gives an indication of the overall _accuracy_ of the classifier.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\" color='#91053d'>**Write a program that evaluates the classifier implemented in the previous exercise on the whole BBC dataset. Print precision, recall and f-measure for each class.\n",
    "Compare different classifiers using different representative documents for the topics**\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<hr style=\" border:none; height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "# true positives for all topics\n",
    "tp = {\"business\": 0, \"entertainment\": 0, \"politics\": 0, \"sport\":0, \"tech\":0}\n",
    "# false positives for all topics\n",
    "fp = {\"business\": 0, \"entertainment\": 0, \"politics\": 0, \"sport\":0, \"tech\":0}\n",
    "# false negatives for all topics\n",
    "fn = {\"business\": 0, \"entertainment\": 0, \"politics\": 0, \"sport\":0, \"tech\":0}\n",
    "\n",
    "print(\"Evaluating classifier...\")\n",
    "for topic in topics:\n",
    "    # Current directory\n",
    "    cd = \"./data/bbc/\"+topic+\"/\"\n",
    "    # For each document under the current topic\n",
    "    for document in os.listdir(cd):\n",
    "        # Get the name of the current document\n",
    "        filename = cd + os.fsdecode(document)\n",
    "        # This is to ensure with do not get any hidden file by mistake \n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # classify the document\n",
    "        predicted = classify(preprocess(sc.textFile(filename), stopwords).distinct(), repwords)\n",
    "        if predicted == topic:\n",
    "            tp[predicted] += 1\n",
    "        else:\n",
    "            fn[topic] += 1\n",
    "            fp[predicted] += 1\n",
    "\n",
    "\n",
    "for topic in topics:\n",
    "    print(\"Class \", topic);\n",
    "    precision = tp[topic] / (tp[topic] + fp[topic])\n",
    "    recall = tp[topic] / (tp[topic] + fn[topic])\n",
    "    fmeasure = 2 * precision * recall / (precision + recall)\n",
    "    print(\"P = \", precision, \" R = \", recall, \" F = \", fmeasure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "name": "BE4-Spark.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
